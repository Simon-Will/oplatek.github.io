---
layout: post
title: Week 2, Dialogue Systems
author: Ondrej Platek
tags: Mff, School, Labs, DS, dialogue systems
---


## Content

- Great homework submission! Thanks!
    - Python & nice code style is a common practice!
    - Deadline Tue 7 AM - so I can review the solutions before next class
    - Unit-tests why?
    - Kaldi & Slurm experience
    - Phonetic examples walk-through
    - Submission formats: github.com or compressed folder
- Barge-in
    - Examples
    - How to detect it? Audio/word/semantic/pragmatic level?
    - Voice Activity Detection (VAD) vs Wake words
    - End-pointing and hesitations
- Meaning abstraction
    - opinionated stance
    - words, sentences
    - speech acts: assertive, directive, commissive, expressive, declarative
        - Look at [DSTC2 dataset](http://camdial.org/~mh521/dstc/)
- Actions
    - replies
    - API calls
        - to database, to robot, to banking system, etc.
        - should we use API? - interface vs self-awareness
- Maxims - is it hard?
    - M. of quantity – don’t give too little/too much information
    - M. of quality – be truthful
    - M. of relation – be relevant
    - M. of manner – be clear
- Grounding and dialogue recovery
- Entropy
    - Definition $$ H(text) = - \sum_{x \in text}{\frac{freq(x)}{len(text)} log_2(\frac{freq(x)}{len(text)})} $$
        - Simplification - Find it!
    - Cross-entropy and LM
        - $$ H(p, q) = -\sum_{x}{p(x) log_2(q(x))}  $$
        - $$ H(text, LM) = -\sum_{x}{ 1/N * log_2(LM(x))}  $$


## Homework

1. (1 point) Implement entropy calculations and compute it for following datasets:
    - [DSTC2 dataset](http://camdial.org/~mh521/dstc/)
    - [Facebook babi tasks 1-6](https://fb-public.app.box.com/s/chnq60iivzv5uckpvj2n2vijlyepze6w). See [github](https://github.com/facebook/bAbI-tasks) for details.
    - [All the news](https://www.kaggle.com/snapcrack/all-the-news) - use just the "Article Content"
    - Use at most first 10000 utterances/sentences if the dataset is large.
    - Describe in 5 sentences properties of each dataset and explain how they correlate with the computed entropy value.
2. (2 point) Train a Language Model and compute cross entropy on [Vystadial dataset](- [Vystadial CZ dataset](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0023-4670-6)
)
     - Recommended toolkit - [KenLM](https://github.com/kpu/kenlm)
         - Read the README and train a model `bin/lmplz -o 5 <text >text.arpa` on Vystadial train set.
         - Compute cross-entropy of the train, dev set and first sentence from dev set. See the [example usage](https://github.com/kpu/kenlm/blob/master/python/example.py)
         - Describe and explain the results in 5 to 10 sentences.

3. BONUS (3 points) Train a wake word model and evaluate it with your voice!
    - Recommended model: [Mycroft precise](https://github.com/MycroftAI/mycroft-precise)
        - [Quickstart](https://github.com/MycroftAI/mycroft-precise/wiki/Training-your-own-wake-word#how-to-train-your-own-wake-word)
    - Write a short summary of what you did and what problems you have faced.
    - Include your dataset with your source code.
    - Include values of [F1 measure](https://en.wikipedia.org/wiki/F1_score) on training, development and test set.
4. BONUS (3 points) Write a conditional language model using RNN (Recurrent Neural Networks).
    - Conditional language model is a decoder RNN with initialized state with additional information.
    - Run the conditional language model on user inputs from DSTC2 dataset.
    - Use previous dialogue state (or part of it) as initialization for your conditional language model.
    - Compare perplexity of vanilla RNN and your conditional implementation on the user inputs.
